{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we load our data (from files containing load sequences) and use it to train a RNN. We take the load history and turn it into sequences of size k, and use it to predict the RRPV for the last address in the sequence. We train the model using the actual RRPV, which is calculated using the full load history."
      ],
      "metadata": {
        "id": "ERpvatNoJp9P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYpo02UmgTCm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import bisect\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "5y_Pm4c1qokr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data\n",
        "batches = []\n",
        "files = (\"drive/MyDrive/loads.txt\", \"drive/MyDrive/loads_concat.txt\") # files containing load sequences\n",
        "for file in files:\n",
        "  with open(file) as raw_data:\n",
        "      curr_batch = []\n",
        "      for line in raw_data:\n",
        "          if line.strip():\n",
        "              curr_batch.append(int(line.strip(), 16))\n",
        "          else:\n",
        "              batches.append(curr_batch)\n",
        "              curr_batch = []\n",
        "\n",
        "long_sequence = []\n",
        "for batch in batches:\n",
        "    long_sequence += batch\n",
        "\n",
        "# 1D sequence of loads\n",
        "original_input = np.array(long_sequence)\n",
        "print(len(original_input))"
      ],
      "metadata": {
        "id": "kbtZJds7rWU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make a map from each address to a list of its occurrences\n",
        "occurrence_map = {}\n",
        "for i in range(len(original_input)):\n",
        "  addr = original_input[i]\n",
        "  if addr not in occurrence_map:\n",
        "    occurrence_map[addr] = []\n",
        "  occurrence_map[addr].append(i)"
      ],
      "metadata": {
        "id": "dFvwdm-wuQ9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# given an address and its position, will return the next instance of the address\n",
        "def find_occurrence_distance(addr, location):\n",
        "  occurrences = occurrence_map[addr]\n",
        "  next_occurrence_index = bisect.bisect(occurrences, location)\n",
        "  if next_occurrence_index >= len(occurrences):\n",
        "    return -1\n",
        "  else:\n",
        "    return occurrences[next_occurrence_index] - location"
      ],
      "metadata": {
        "id": "PT9vtq9OvOd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to normalize target values\n",
        "#input: number of loads till target is loaded [1, inf]\n",
        "#output: floating pt greater than eq to zero (greater value indicates sooner access)\n",
        "def normalize(num_loads):\n",
        "  #return (num_loads if num_loads >= 0 else 100000)\n",
        "  if num_loads == -1:\n",
        "    return 1/(1+math.log10(130000))\n",
        "  return 1/(1+math.log10(num_loads))"
      ],
      "metadata": {
        "id": "ajQcO8-uPeCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_seq = []\n",
        "target_seq = []\n",
        "target_seq_raw = []\n",
        "\n",
        "input_engineered = []\n",
        "\n",
        "k = 100\n",
        "# parition into arrays of size k\n",
        "seq_start = 0\n",
        "seq_end = (seq_start + k) - 1\n",
        "while (seq_end < len(original_input)):\n",
        "  #pull last k loads\n",
        "  load_history = original_input[seq_start:seq_end+1]\n",
        "  load_addr = load_history[-1]\n",
        "\n",
        "  # history is encoded - 1 if value == target address, 0 otherwsise\n",
        "  load_history = [1 if x == load_addr else 0 for x in load_history]\n",
        "\n",
        "  # remove values with no histrory, not useful for training\n",
        "  if sum(load_history) == 1:\n",
        "    seq_start += 1\n",
        "    seq_end += 1\n",
        "    continue\n",
        "  input_seq.append(load_history)\n",
        "  # use occurrence distance as target\n",
        "  next_occurrence = find_occurrence_distance(load_addr, seq_end)\n",
        "  if next_occurrence == -1:\n",
        "      seq_start += 1\n",
        "      seq_end += 1\n",
        "\n",
        "  target_seq.append(normalize(next_occurrence))\n",
        "  target_seq_raw.append(next_occurrence)\n",
        "  seq_start += 1\n",
        "  seq_end += 1\n",
        "\n",
        "input_seq = np.array(input_seq)\n",
        "target_seq = np.array(target_seq).reshape(len(target_seq),1)\n",
        "print(target_seq)\n",
        "print(target_seq.shape)"
      ],
      "metadata": {
        "id": "Z9U24AAvtXW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use gpu\n",
        "is_cuda = torch.cuda.is_available()\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "Ib-fmhzimWGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        # Defining some parameters\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        #Defining the layers\n",
        "        # RNN Layer\n",
        "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)   \n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        batch_size = 50\n",
        "\n",
        "\n",
        "        #Initializing hidden state for first input using method defined below\n",
        "        hidden = self.init_hidden()\n",
        "\n",
        "        # Passing in the input and hidden state into the model and obtaining outputs\n",
        "        out, hidden = self.rnn(x, hidden)\n",
        "        \n",
        "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
        "        out = out.contiguous().view(-1, self.hidden_dim)\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        return out, hidden\n",
        "    \n",
        "    def init_hidden(self):\n",
        "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
        "        hidden = torch.zeros(self.n_layers, self.hidden_dim).to(device)\n",
        "         # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
        "        return hidden"
      ],
      "metadata": {
        "id": "pRmGa2Ciy6k_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model with hyperparameters\n",
        "model = Model(input_size=k, output_size=1, hidden_dim=10, n_layers=1)\n",
        "# We'll also set the model to the device defined earlier (default is CPU)\n",
        "model = model.to(device)\n",
        "\n",
        "# Define hyperparameters\n",
        "n_epochs = 10\n",
        "lr=0.01\n",
        "\n",
        "# Define Loss, Optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n"
      ],
      "metadata": {
        "id": "xXefpaDgWOVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split into training and testing data\n",
        "X_train, X_test, y_train, y_test = train_test_split(input_seq, target_seq, random_state = 20, shuffle=False, train_size = 0.9)\n",
        "\n",
        "X_train = torch.from_numpy(X_train)\n",
        "y_train = torch.from_numpy(y_train)\n",
        "X_test = torch.from_numpy(X_test)\n",
        "y_test = torch.from_numpy(y_test)\n",
        "\n",
        "X_train = X_train.to(torch.float32)\n",
        "y_train = y_train.to(torch.float32)\n",
        "X_test = X_test.to(torch.float32)\n",
        "y_test = y_test.to(torch.float32)\n",
        "\n",
        "X_train = torch.nn.functional.normalize(X_train)\n",
        "X_test = torch.nn.functional.normalize(X_test)"
      ],
      "metadata": {
        "id": "pMrTsVOvUDXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = TensorDataset(X_train,y_train)\n",
        "test = TensorDataset(X_test,y_test)\n",
        "\n",
        "train_loader = DataLoader(train, batch_size = 100, shuffle = False)\n",
        "test_loader = DataLoader(test, batch_size = 10000, shuffle = False)"
      ],
      "metadata": {
        "id": "QllpZ5_odzuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Run\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "\n",
        "    for (features, target) in train_loader:\n",
        "      features = Variable(features)\n",
        "      target = Variable(target)\n",
        "\n",
        "      features = features.to(device)\n",
        "\n",
        "      optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
        "      output, hidden = model(features)\n",
        "      output = output.to(device)\n",
        "      target = target.to(device)\n",
        "      loss = criterion(output, target.view(-1, 1))\n",
        "      loss.backward() # Does backpropagation and calculates gradients\n",
        "      optimizer.step() # Updates the weights accordingly\n",
        "    \n",
        "    print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
        "    print(\"Loss: {:.4f}\".format(loss.item()))"
      ],
      "metadata": {
        "id": "JqsAfAldXdHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#validate\n",
        "\n",
        "for (features, target) in test_loader:\n",
        "  features = Variable(features)\n",
        "  target = Variable(target)\n",
        "\n",
        "  features = features.to(device)\n",
        "  outputs = model(features)\n",
        "\n",
        "target = target.cpu().detach().numpy().reshape(len(target))\n",
        "outputs = outputs[0].cpu().detach().numpy().reshape(len(target))\n",
        "plt.scatter(target,outputs)\n",
        "\n",
        "identity_line = np.linspace(max(min(target), min(outputs)),min(max(target), max(outputs)))\n",
        "plt.plot(identity_line, identity_line, color=\"black\", linewidth=1.0)\n",
        "\n",
        "print(outputs)\n",
        "print(np.corrcoef(target, outputs))"
      ],
      "metadata": {
        "id": "xlbaEw9fXlny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saves model to drive file that is used by cache module for prediction\n",
        "torch.save(model.state_dict(), \"drive/MyDrive/model\")"
      ],
      "metadata": {
        "id": "xdLR_2q4-PC8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}